{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyuUuP9Mz3-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bdc75c0-79fb-42c0-d622-56b5e87c2749",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished count: 10000\n",
            "Finished count: 20000\n",
            "Finished count: 30000\n",
            "Finished count: 40000\n",
            "Finished count: 50000\n",
            "Finished count: 60000\n",
            "Finished count: 70000\n",
            "Finished count: 80000\n",
            "Finished count: 90000\n",
            "Finished count: 100000\n",
            "Finished count: 110000\n",
            "Finished count: 120000\n",
            "Finished count: 130000\n",
            "Finished count: 140000\n",
            "Finished count: 150000\n",
            "Finished count: 160000\n",
            "Finished count: 170000\n",
            "Finished count: 180000\n",
            "Finished count: 190000\n",
            "Finished count: 200000\n",
            "Finished count: 210000\n",
            "Finished count: 220000\n",
            "Finished count: 230000\n",
            "Finished count: 240000\n",
            "Finished count: 250000\n",
            "Finished count: 260000\n",
            "Finished count: 270000\n",
            "Finished count: 280000\n",
            "Finished count: 290000\n",
            "Finished count: 300000\n",
            "Finished count: 310000\n",
            "Finished count: 320000\n",
            "Finished count: 330000\n",
            "Finished count: 340000\n",
            "Finished count: 350000\n",
            "Finished count: 360000\n",
            "Finished count: 370000\n",
            "Finished count: 380000\n",
            "Finished count: 390000\n",
            "Finished count: 400000\n",
            "Finished count: 410000\n",
            "Finished count: 420000\n",
            "Finished count: 430000\n",
            "Finished count: 440000\n",
            "Finished count: 450000\n",
            "Finished count: 460000\n",
            "Finished count: 470000\n",
            "Finished count: 480000\n",
            "Finished count: 490000\n",
            "Finished count: 500000\n",
            "Finished count: 510000\n",
            "Finished count: 520000\n",
            "Finished count: 530000\n",
            "Finished count: 540000\n",
            "Finished count: 550000\n",
            "Finished count: 560000\n",
            "Finished count: 570000\n",
            "Finished count: 580000\n",
            "Finished count: 590000\n",
            "Finished count: 600000\n",
            "Finished count: 610000\n",
            "Finished count: 620000\n",
            "Finished count: 630000\n",
            "Finished count: 640000\n",
            "Finished count: 650000\n",
            "Finished count: 660000\n",
            "Finished count: 670000\n",
            "Finished count: 680000\n",
            "Finished count: 690000\n",
            "Finished count: 700000\n",
            "Finished count: 710000\n",
            "Finished count: 720000\n",
            "Finished count: 730000\n",
            "Finished count: 740000\n",
            "Finished count: 750000\n",
            "Finished count: 760000\n",
            "Finished count: 770000\n",
            "Finished count: 780000\n",
            "Finished count: 790000\n",
            "Finished count: 800000\n",
            "Finished count: 810000\n",
            "Finished count: 820000\n",
            "Finished count: 830000\n",
            "Finished count: 840000\n",
            "Finished count: 850000\n",
            "Finished count: 860000\n",
            "Finished count: 870000\n",
            "Finished count: 880000\n",
            "Finished count: 890000\n",
            "Finished count: 900000\n",
            "Finished count: 910000\n",
            "Finished count: 920000\n",
            "Finished count: 930000\n",
            "Finished count: 940000\n",
            "Finished count: 950000\n",
            "Finished count: 960000\n",
            "Finished count: 970000\n",
            "Finished count: 980000\n",
            "Finished count: 990000\n",
            "Finished count: 1000000\n",
            "Finished count: 1010000\n",
            "Finished count: 1020000\n",
            "Finished count: 1030000\n",
            "Finished count: 1040000\n",
            "Finished count: 1050000\n",
            "Finished count: 1060000\n",
            "Finished count: 1070000\n",
            "Finished count: 1080000\n",
            "Finished count: 1090000\n",
            "Finished count: 1100000\n",
            "Finished count: 1110000\n",
            "Finished count: 1120000\n",
            "Finished count: 1130000\n",
            "Finished count: 1140000\n",
            "Finished count: 1150000\n",
            "Finished count: 1160000\n",
            "Finished count: 1170000\n",
            "Finished count: 1180000\n",
            "Finished count: 1190000\n",
            "Finished count: 1200000\n",
            "Finished count: 1210000\n",
            "Finished count: 1220000\n",
            "Finished count: 1230000\n",
            "Finished count: 1240000\n",
            "Finished count: 1250000\n",
            "Finished count: 1260000\n",
            "Finished count: 1270000\n",
            "Finished count: 1280000\n",
            "Finished count: 1290000\n",
            "Finished count: 1300000\n",
            "Finished count: 1310000\n",
            "Finished count: 1320000\n",
            "Finished count: 1330000\n",
            "Finished count: 1340000\n",
            "Finished count: 1350000\n",
            "Finished count: 1360000\n",
            "Finished count: 1370000\n",
            "Finished count: 1380000\n",
            "Finished count: 1390000\n",
            "Finished count: 1400000\n",
            "Finished count: 1410000\n",
            "Finished count: 1420000\n",
            "Finished count: 1430000\n",
            "Finished count: 1440000\n",
            "Finished count: 1450000\n",
            "Finished count: 1460000\n",
            "Finished count: 1470000\n",
            "Finished count: 1480000\n",
            "Finished count: 1490000\n",
            "Finished count: 1500000\n",
            "Finished count: 1510000\n",
            "Finished count: 1520000\n",
            "Finished count: 1530000\n",
            "Finished count: 1540000\n",
            "Finished count: 1550000\n",
            "Finished count: 1560000\n",
            "Finished count: 1570000\n",
            "Finished count: 1580000\n",
            "Finished count: 1590000\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing\n",
        "import csv\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import nltk\n",
        "\n",
        "dir = '/content/drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "# data source: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
        "\"\"\"\n",
        "It contains the following 6 fields:\n",
        "\n",
        "target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
        "\n",
        "ids: The id of the tweet ( 2087)\n",
        "\n",
        "date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
        "\n",
        "flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
        "\n",
        "user: the user that tweeted (robotickilldozr)\n",
        "\n",
        "text: the text of the tweet (Lyx is cool)\n",
        "\"\"\"\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "import re\n",
        "# preprocess the data\n",
        "with open(dir+'labeled_tweets.csv', mode='r', encoding='utf-8', errors='replace') as src:\n",
        "  with open(dir+\"cleaned_tweets.csv\", mode='w', encoding='utf-8', errors='replace') as dst:\n",
        "    reader = csv.reader(src)\n",
        "    writer = csv.writer(dst)\n",
        "    tk = TweetTokenizer(strip_handles=True, preserve_case=False)\n",
        "    nltk.download('stopwords')\n",
        "    stopwords = set(stopwords.words('english'))\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    # Extract the tweet text and sentiment labels\n",
        "    count = 0\n",
        "    for row in reader:\n",
        "      text = row[5]\n",
        "      text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "      tokens = tk.tokenize(text)\n",
        "      # Remove stopwords, hyperlinks, user mentions, and stem the tokens\n",
        "      tokens = [stemmer.stem(token) for token in tokens\n",
        "                  if token not in stopwords]\n",
        "      # store label and cleaned data for analysis\n",
        "      if tokens != []:\n",
        "        writer.writerow(['1' if int(row[0]) > 0 else '0', ' '.join(tokens)])\n",
        "        count += 1\n",
        "        if (count % 50000 == 0):\n",
        "          print(f\"Finished count: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dir = '/content/drive/My Drive/Colab Notebooks/'\n",
        "np.random.seed(0)\n",
        "df = pd.read_csv(dir+\"cleaned_tweets.csv\", header=None)\n",
        "df.columns = ['label', 'text']\n",
        "X = df['text']\n",
        "y = df['label']"
      ],
      "metadata": {
        "id": "eiY-HuOv0jCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Vector\n",
        "# hyperparameter\n",
        "EMBEDDING_DIM = 300\n",
        "# \"\"\"\n",
        "# Load Pre-trained GloVe Vectors\n",
        "# Ref: https://nlp.stanford.edu/projects/glove/\n",
        "# Return:\n",
        "#     wv_from_bin: All 400000 embeddings, each length EMBEDDING_DIM\n",
        "# \"\"\"\n",
        "import gensim\n",
        "documents = [_text.split() for _text in X]\n",
        "\n",
        "glove_vectors = gensim.models.word2vec.Word2Vec(vector_size=EMBEDDING_DIM,\n",
        "                                            window=7,\n",
        "                                            min_count=10,\n",
        "                                            workers=8)\n",
        "glove_vectors.build_vocab(documents)\n",
        "words = glove_vectors.wv\n",
        "vocab_size = len(words)\n",
        "print(\"Vocab size\", vocab_size)\n",
        "\n",
        "glove_vectors.train(documents, total_examples=len(documents), epochs=32)\n",
        "glove_vectors.save(dir+'word2vec.model')"
      ],
      "metadata": {
        "id": "A8SYI6BLL0xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and Encoding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "MAX_LENGTH= max([len(s.split()) for s in X])\n",
        "print(f\"Max Length: {MAX_LENGTH}\")\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"example data: {X[0]}\")\n",
        "\n",
        "seq = tokenizer.texts_to_sequences(X)\n",
        "print(f\"example sequence: {seq[0]}\")\n",
        "X = pad_sequences(tokenizer.texts_to_sequences(X),\n",
        "                        maxlen = MAX_LENGTH, padding='post')\n",
        "print(f\"example pad: {X[0]}\")\n",
        "\n",
        "\n",
        "TRAIN_SIZE = 0.8\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y)\n",
        "y = encoder.transform(y)\n",
        "y = y.reshape(-1,1)\n",
        "\n",
        "print(f\"Total: {len(X)} samples\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=TRAIN_SIZE, shuffle=True, random_state=0)\n",
        "print(f\"Training data: {len(X_train)} samples\")\n",
        "print(f\"Testing data: {len(X_test)} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plyuAVvSVGFA",
        "outputId": "dcdc0b85-4f97-45d5-eaba-c2035445313b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Length: 50\n",
            "Vocabulary size: 283761\n",
            "example data: awww bummer shoulda got david carr third day\n",
            "example sequence: [368, 1047, 3061, 11, 703, 7476, 1689, 3]\n",
            "example pad: [ 368 1047 3061   11  703 7476 1689    3    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0]\n",
            "Total: 1592328 samples\n",
            "Training data: 1273862 samples\n",
            "Testing data: 318466 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding layer\n",
        "from gensim.models import KeyedVectors\n",
        "glove_vectors = KeyedVectors.load(dir+'word2vec.model')\n",
        "\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "words = glove_vectors.wv.key_to_index\n",
        "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if word in words:\n",
        "        embedding_matrix[i] = glove_vectors.wv[word]\n",
        "print(f\"Embedding Matrix shape: {embedding_matrix.shape}\")\n",
        "\n",
        "import tensorflow as tf\n",
        "embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          EMBEDDING_DIM,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          trainable=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAUyuWhE1deY",
        "outputId": "ae34d3e0-36ff-46ce-9312-b9066792366c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Matrix shape: (283761, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    embedding_layer,\n",
        "    Dropout(0.5),\n",
        "    LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "\n",
        "ReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n",
        "                                     monitor = 'val_loss',\n",
        "                                     patience = 5,\n",
        "                                     verbose = 1,\n",
        "                                      cooldown=0)\n",
        "EarlyStopping = EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5, mode='max')\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "G13sgzHe3kvl",
        "outputId": "18541dbf-92f8-4cf3-e356-e8c8223b2cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │      \u001b[38;5;34m85,128,300\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">85,128,300</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m85,128,300\u001b[0m (324.74 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">85,128,300</span> (324.74 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m85,128,300\u001b[0m (324.74 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">85,128,300</span> (324.74 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024\n",
        "EPOCHS = 15\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"First sample in X_train:\", X_train[0])\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(X_test, y_test), callbacks=[ReduceLROnPlateau, EarlyStopping])\n",
        "model.save(dir+'LSTM.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DDQiZKggVsn",
        "outputId": "16308fa8-9568-4e60-9932-3fca3be58d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (1273862, 50)\n",
            "First sample in X_train: [ 339  337 1259    3    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0]\n",
            "Epoch 1/15\n",
            "\u001b[1m1245/1245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 233ms/step - accuracy: 0.7028 - loss: 0.5643 - val_accuracy: 0.7786 - val_loss: 0.4679 - learning_rate: 0.0010\n",
            "Epoch 2/15\n",
            "\u001b[1m1245/1245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 230ms/step - accuracy: 0.7660 - loss: 0.4869 - val_accuracy: 0.7849 - val_loss: 0.4547 - learning_rate: 0.0010\n",
            "Epoch 3/15\n",
            "\u001b[1m1245/1245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 230ms/step - accuracy: 0.7821 - loss: 0.4602 - val_accuracy: 0.7906 - val_loss: 0.4458 - learning_rate: 0.0010\n",
            "Epoch 4/15\n",
            "\u001b[1m1245/1245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 231ms/step - accuracy: 0.7944 - loss: 0.4399 - val_accuracy: 0.7908 - val_loss: 0.4448 - learning_rate: 0.0010\n",
            "Epoch 5/15\n",
            "\u001b[1m1245/1245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 232ms/step - accuracy: 0.8066 - loss: 0.4192 - val_accuracy: 0.7910 - val_loss: 0.4478 - learning_rate: 0.0010\n",
            "Epoch 6/15\n",
            "\u001b[1m1245/1245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 239ms/step - accuracy: 0.8159 - loss: 0.4017 - val_accuracy: 0.7891 - val_loss: 0.4586 - learning_rate: 0.0010\n",
            "Epoch 7/15\n",
            "\u001b[1m1245/1245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 230ms/step - accuracy: 0.8231 - loss: 0.3877 - val_accuracy: 0.7862 - val_loss: 0.4706 - learning_rate: 0.0010\n",
            "Epoch 8/15\n",
            "\u001b[1m1245/1245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 230ms/step - accuracy: 0.8274 - loss: 0.3778 - val_accuracy: 0.7848 - val_loss: 0.4874 - learning_rate: 0.0010\n",
            "Epoch 9/15\n",
            "\u001b[1m1245/1245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.8310 - loss: 0.3689\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\u001b[1m1245/1245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 230ms/step - accuracy: 0.8310 - loss: 0.3689 - val_accuracy: 0.7853 - val_loss: 0.5023 - learning_rate: 0.0010\n",
            "Epoch 10/15\n",
            "\u001b[1m1245/1245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 230ms/step - accuracy: 0.8359 - loss: 0.3581 - val_accuracy: 0.7842 - val_loss: 0.5215 - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model(dir+'LSTM.keras')\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O823rdlZz-8",
        "outputId": "b6bf9417-f0ea-41ec-96b9-329e148e608d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 14 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m9953/9953\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 35ms/step - accuracy: 0.7801 - loss: 0.4809\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.48045244812965393, 0.7805857062339783]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}